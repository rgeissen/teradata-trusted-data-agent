# src/trusted_data_agent/agent/executor.py
import re
import json
import logging
from enum import Enum, auto

from trusted_data_agent.agent.formatter import OutputFormatter
from trusted_data_agent.core import session_manager
from trusted_data_agent.mcp import adapter as mcp_adapter
from trusted_data_agent.llm import handler as llm_handler

app_logger = logging.getLogger("quart.app")

class AgentState(Enum):
    """
    Represents the current state of the agent's execution flow.
    """
    DECIDING = auto()
    EXECUTING_TOOL = auto()
    SUMMARIZING = auto()
    DONE = auto()
    ERROR = auto()

def _format_sse(data: dict, event: str = None) -> str:
    """
    Formats a dictionary into a Server-Sent Event (SSE) string.

    Args:
        data: The dictionary payload.
        event: An optional event name.

    Returns:
        A string formatted for SSE.
    """
    msg = f"data: {json.dumps(data)}\n"
    if event is not None:
        msg += f"event: {event}\n"
    return f"{msg}\n"

def _evaluate_inline_math(json_str: str) -> str:
    """
    Finds and evaluates simple mathematical expressions within a JSON string.
    This is used to handle cases where the LLM might generate chart parameters
    with inline calculations (e.g., "width": "100 * 2").

    Args:
        json_str: The JSON string to process.

    Returns:
        The JSON string with evaluated math expressions.
    """
    math_expr_pattern = re.compile(r'\b(\d+\.?\d*)\s*([+\-*/])\s*(\d+\.?\d*)\b')
    while True:
        match = math_expr_pattern.search(json_str)
        if not match: break
        num1_str, op, num2_str = match.groups()
        try:
            num1, num2 = float(num1_str), float(num2_str)
            result = 0
            if op == '+': result = num1 + num2
            elif op == '-': result = num1 - num2
            elif op == '*': result = num1 * num2
            elif op == '/': result = num1 / num2
            json_str = json_str.replace(match.group(0), str(result), 1)
        except (ValueError, ZeroDivisionError):
            # Ignore expressions that fail to evaluate
            break
    return json_str

class PlanExecutor:
    """
    Manages the step-by-step execution of a plan generated by the LLM.
    It handles the agent's state, tool execution, data collection, and final summary generation.
    """
    def __init__(self, session_id: str, initial_instruction: str, original_user_input: str, dependencies: dict):
        self.session_id = session_id
        self.original_user_input = original_user_input
        self.state = AgentState.DECIDING
        self.next_action_str = initial_instruction
        self.collected_data = []
        self.max_steps = 40  # Safety break to prevent infinite loops
        self.active_prompt_plan = None
        self.active_prompt_name = None
        self.current_command = None
        self.iteration_context = None
        self.dependencies = dependencies
        self.tool_constraints_cache = {} # Cache for inferred tool constraints

    async def run(self):
        """
        Runs the main execution loop of the agent. This generator function
        yields Server-Sent Events (SSE) for each step of the process.
        """
        for i in range(self.max_steps):
            if self.state in [AgentState.DONE, AgentState.ERROR]:
                break
            try:
                if self.state == AgentState.DECIDING:
                    yield _format_sse({"step": "Assistant has decided on an action", "details": self.next_action_str}, "llm_thought")
                    async for event in self._handle_deciding():
                        yield event
                elif self.state == AgentState.EXECUTING_TOOL:
                    tool_name = self.current_command.get("tool_name")
                    tool_scopes = self.dependencies['STATE'].get('tool_scopes', {})
                    # --- MODIFICATION: Route to column iteration logic if scope matches ---
                    if tool_scopes.get(tool_name) == 'column':
                        async for event in self._execute_column_iteration():
                            yield event
                    else:
                        async for event in self._execute_standard_tool():
                            yield event
                    # --- END MODIFICATION ---
                elif self.state == AgentState.SUMMARIZING:
                    async for event in self._handle_summarizing():
                        yield event
            except Exception as e:
                app_logger.error(f"Error in state {self.state.name}: {e}", exc_info=True)
                self.state = AgentState.ERROR
                yield _format_sse({"error": "An error occurred during execution.", "details": str(e)}, "error")
        
        if self.state not in [AgentState.DONE, AgentState.ERROR]:
            async for event in self._handle_summarizing():
                yield event

    async def _handle_deciding(self):
        """
        Parses the LLM's next action string to determine whether to execute a tool,
        follow a prompt, or generate the final answer.
        """
        if re.search(r'FINAL_ANSWER:', self.next_action_str, re.IGNORECASE):
            self.state = AgentState.SUMMARIZING
            return

        json_match = re.search(r"```json\s*\n(.*?)\n\s*```", self.next_action_str, re.DOTALL)
        if not json_match:
            if self.iteration_context:
                ctx = self.iteration_context
                current_item_name = ctx["items"][ctx["item_index"]]
                ctx["results_per_item"][current_item_name].append(self.next_action_str)
                ctx["action_count_for_item"] += 1
                async for event in self._get_next_action_from_llm():
                    yield event
                return

            app_logger.warning(f"LLM response not a tool command or FINAL_ANSWER. Summarizing. Response: {self.next_action_str}")
            self.state = AgentState.SUMMARIZING
            return

        command_str = json_match.group(1).strip()
        
        try:
            temp_command = json.loads(command_str)
            tool_name = temp_command.get("tool_name")
            mcp_charts = self.dependencies['STATE'].get('mcp_charts', {})

            if tool_name in mcp_charts:
                corrected_command_str = _evaluate_inline_math(command_str)
                command = json.loads(corrected_command_str)
                if command_str != corrected_command_str:
                    app_logger.info(f"Corrected inline math in chart JSON. Corrected string: {corrected_command_str}")
            else:
                command = temp_command
        
        except (json.JSONDecodeError, KeyError) as e:
            app_logger.error(f"JSON parsing failed or 'tool_name' key missing. Error: {e}. Original string was: {command_str}")
            raise e
            
        if "tool_name" in command:
            t_name = command["tool_name"]
            mcp_tools = self.dependencies['STATE'].get('mcp_tools', {})
            mcp_prompts = self.dependencies['STATE'].get('mcp_prompts', {})
            if t_name not in mcp_tools and t_name in mcp_prompts:
                app_logger.warning(f"LLM hallucinated tool '{t_name}'. Correcting to prompt.")
                yield _format_sse({
                    "step": "System Correction",
                    "details": f"LLM incorrectly used 'tool_name' for a prompt. Corrected '{t_name}' to be a prompt.",
                    "type": "workaround"
                })
                command["prompt_name"] = command.pop("tool_name")
            
        self.current_command = command
        
        if "prompt_name" in command:
            prompt_name = command.get("prompt_name")
            self.active_prompt_name = prompt_name
            arguments = command.get("arguments", command.get("parameters", {}))
            
            if 'db_name' in arguments and 'database_name' not in arguments:
                arguments['database_name'] = arguments['db_name']

            mcp_client = self.dependencies['STATE'].get('mcp_client')
            if not mcp_client:
                raise RuntimeError("MCP client is not connected.")

            try:
                get_prompt_result = None
                async with mcp_client.session("teradata_mcp_server") as temp_session:
                    get_prompt_result = await temp_session.get_prompt(name=prompt_name)
                
                if get_prompt_result is None:
                    raise ValueError("Prompt retrieval from MCP server returned None.")

                prompt_text = get_prompt_result.content.text if hasattr(get_prompt_result, 'content') and hasattr(get_prompt_result.content, 'text') else str(get_prompt_result)
                
                try:
                    self.active_prompt_plan = prompt_text.format(**arguments)
                except KeyError as e:
                    missing_key = e.args[0]
                    app_logger.warning(f"Prompt formatting failed for key '{missing_key}'. Attempting to extract from user input.")
                    
                    match = re.search(r'\b([A-Z0-9_]+_db)\b', self.original_user_input, re.IGNORECASE)
                    if match:
                        yield _format_sse({
                            "step": "System Correction",
                            "details": f"LLM failed to provide database name for prompt. Attempting to recover it from user's original query.",
                            "type": "workaround"
                        })
                        extracted_db_name = match.group(1)
                        arguments['db_name'] = extracted_db_name
                        arguments['database_name'] = extracted_db_name
                        app_logger.info(f"Extracted '{extracted_db_name}' from user input for missing key. Retrying format.")
                        self.active_prompt_plan = prompt_text.format(**arguments)
                    else:
                        raise e

                yield _format_sse({"step": f"Executing Prompt: {prompt_name}", "details": self.active_prompt_plan, "prompt_name": prompt_name}, "prompt_selected")

                async for event in self._get_next_action_from_llm():
                    yield event
            except Exception as e:
                app_logger.error(f"Failed to get or process prompt '{prompt_name}': {e}", exc_info=True)
                raise RuntimeError(f"Could not retrieve the plan for prompt '{prompt_name}'.") from e

        elif "tool_name" in command:
            self.state = AgentState.EXECUTING_TOOL
        else:
            self.state = AgentState.SUMMARIZING
    
    async def _execute_standard_tool(self):
        """
        Invokes a standard (non-iterative) tool via the MCP adapter and
        handles the result.
        """
        yield _format_sse({"step": "Tool Execution Intent", "details": self.current_command}, "tool_result")
        tool_result = await mcp_adapter.invoke_mcp_tool(self.dependencies['STATE'], self.current_command)
        
        if 'notification' in self.current_command:
            yield _format_sse({
                "step": "System Notification", 
                "details": self.current_command['notification'],
                "type": "workaround"
            })
            del self.current_command['notification']

        tool_result_str = ""
        if isinstance(tool_result, dict) and "error" in tool_result:
            error_details = tool_result.get("data", tool_result.get("error"))
            tool_result_str = json.dumps({ "tool_name": self.current_command.get("tool_name"), "tool_output": { "status": "error", "error_message": error_details } })
        else:
            tool_result_str = json.dumps({"tool_name": self.current_command.get("tool_name"), "tool_output": tool_result})
            if isinstance(tool_result, dict) and tool_result.get("type") == "chart":
                if self.collected_data:
                    app_logger.info("Chart generated. Removing previous data source from collected data to avoid duplicate display.")
                    self.collected_data.pop()
            self.collected_data.append(tool_result)

        if isinstance(tool_result, dict) and tool_result.get("error") == "parameter_mismatch":
            yield _format_sse({"details": tool_result}, "request_user_input")
            self.state = AgentState.ERROR
            return

        yield _format_sse({"step": "Tool Execution Result", "details": tool_result, "tool_name": self.current_command.get("tool_name")}, "tool_result")

        if self.active_prompt_plan and not self.iteration_context:
            plan_text = self.active_prompt_plan.lower()
            is_iterative_plan = any(keyword in plan_text for keyword in ["cycle through", "for each", "iterate"])
            
            if is_iterative_plan and self.current_command.get("tool_name") == "base_tableList" and isinstance(tool_result, dict) and tool_result.get("status") == "success":
                items_to_iterate = [res.get("TableName") for res in tool_result.get("results", []) if res.get("TableName")]
                if items_to_iterate:
                    self.iteration_context = { "items": items_to_iterate, "item_index": 0, "action_count_for_item": 0, "results_per_item": {item: [] for item in items_to_iterate} }
                    yield _format_sse({"step": "Starting Multi-Step Iteration", "details": f"Plan requires processing {len(items_to_iterate)} items."})
        
        yield _format_sse({"step": "Thinking about the next action...", "details": "The agent is reasoning based on the current context."})
        async for event in self._get_next_action_from_llm(tool_result_str=tool_result_str):
            yield event

    # --- NEW: Method to get and cache tool constraints ---
    async def _get_tool_constraints(self, tool_name: str):
        """
        Infers and caches the operational constraints of a tool (e.g., data type)
        by using the LLM. This is a generator function that yields status updates
        and finally yields the constraint dictionary.
        """
        if tool_name in self.tool_constraints_cache:
            yield self.tool_constraints_cache[tool_name]
            return

        mcp_tools = self.dependencies['STATE'].get('mcp_tools', {})
        tool_definition = mcp_tools.get(tool_name)
        
        constraints = {}
        
        if tool_definition:
            prompt = (
                "You are a tool analyzer. Based on the tool's name and description, determine if its `col_name` "
                "argument is intended for 'numeric' types, 'character' types, or 'any' type. "
                "Respond with a single, raw JSON object with one key, 'dataType', and the value 'numeric', 'character', or 'any'.\n\n"
                f"Tool Name: `{tool_definition.name}`\n"
                f"Tool Description: \"{tool_definition.description}\"\n\n"
                "Example response for a statistical tool: {\"dataType\": \"numeric\"}"
            )
            
            yield _format_sse({
                "step": f"Inferring constraints for tool: {tool_name}",
                "details": "Asking LLM to analyze tool requirements...",
                "type": "workaround"
            })

            response_text, _, _ = await llm_handler.call_llm_api(
                self.dependencies['STATE']['llm'], 
                prompt, 
                raise_on_error=True,
                system_prompt_override="You are a JSON-only responding assistant."
            )
            
            try:
                match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if not match:
                    raise ValueError("LLM did not return valid JSON for constraints.")
                
                constraints = json.loads(match.group(0))
                app_logger.info(f"Inferred constraints for tool '{tool_name}': {constraints}")
            except (json.JSONDecodeError, ValueError) as e:
                app_logger.error(f"Failed to infer constraints for {tool_name}: {e}. Assuming no constraints.")
                constraints = {}
        
        self.tool_constraints_cache[tool_name] = constraints
        yield constraints
    # --- END NEW ---

    # --- NEW: Adaptive logic for column-scoped tools ---
    async def _execute_column_iteration(self):
        """
        Handles the execution of tools that operate on a per-column basis.
        This version is adaptive: it fetches column metadata first and only
        runs tools on columns with a compatible data type based on LLM-inferred constraints.
        """
        base_command = self.current_command
        tool_name = base_command.get("tool_name")
        base_args = base_command.get("arguments", base_command.get("parameters", {}))
        
        db_name = base_args.get("db_name")
        table_name = base_args.get("table_name")

        if table_name and '.' in table_name and not db_name:
            db_name, table_name = table_name.split('.', 1)
            app_logger.info(f"Parsed db_name '{db_name}' from fully qualified table_name.")

        specific_column = base_args.get("col_name") or base_args.get("column_name")
        if specific_column:
            yield _format_sse({"step": "Tool Execution Intent", "details": base_command}, "tool_result")
            col_result = await mcp_adapter.invoke_mcp_tool(self.dependencies['STATE'], base_command)
            
            if 'notification' in self.current_command:
                yield _format_sse({
                    "step": "System Notification", 
                    "details": self.current_command['notification'],
                    "type": "workaround"
                })
                del self.current_command['notification']

            if isinstance(col_result, dict) and col_result.get("error") == "parameter_mismatch":
                yield _format_sse({"details": col_result}, "request_user_input")
                self.state = AgentState.ERROR
                return

            yield _format_sse({"step": f"Tool Execution Result for column: {specific_column}", "details": col_result, "tool_name": tool_name}, "tool_result")
            self.collected_data.append(col_result)
            tool_result_str = json.dumps({"tool_name": tool_name, "tool_output": col_result})
            yield _format_sse({"step": "Thinking about the next action...", "details": "Single column execution complete. Resuming main plan."})
            async for event in self._get_next_action_from_llm(tool_result_str=tool_result_str):
                yield event
            return

        yield _format_sse({
            "step": f"Adaptive column tool detected: {tool_name}", 
            "details": "Fetching column metadata to determine compatibility.",
            "type": "workaround"
        })
        cols_command = {"tool_name": "base_columnDescription", "arguments": {"db_name": db_name, "obj_name": table_name}}
        cols_result = await mcp_adapter.invoke_mcp_tool(self.dependencies['STATE'], cols_command)

        if not (cols_result and isinstance(cols_result, dict) and cols_result.get('status') == 'success' and cols_result.get('results')):
            raise ValueError(f"Failed to retrieve column list for iteration. Response: {cols_result}")
        
        all_columns_metadata = cols_result.get('results', [])
        all_column_results = []
        
        tool_constraints = None
        async for event in self._get_tool_constraints(tool_name):
            if isinstance(event, dict):
                tool_constraints = event
            else:
                yield event
        
        required_type = tool_constraints.get("dataType") if tool_constraints else None

        for column_info in all_columns_metadata:
            col_name = column_info.get("ColumnName")
            
            possible_keys = ["Datatype", "ColumnType", "Type", "DataType"]
            col_type = ""
            for key in possible_keys:
                if key in column_info:
                    col_type = (column_info[key] or "").upper()
                    break

            if required_type:
                is_numeric_column = any(t in col_type for t in ["INT", "NUM", "DEC", "FLOAT", "BYTEINT", "SMALLINT", "BIGINT"])
                is_char_column = any(t in col_type for t in ["CHAR", "VARCHAR"])
                
                should_skip = False
                if required_type == "numeric" and not is_numeric_column:
                    should_skip = True
                elif required_type == "character" and not is_char_column:
                    should_skip = True

                if should_skip:
                    skip_reason = f"Tool '{tool_name}' requires a {required_type} column, but '{col_name}' is of type {col_type}."
                    app_logger.info(f"SKIPPING: {skip_reason}")
                    skip_result = {
                        "status": "skipped",
                        "reason": skip_reason,
                        "metadata": {"tool_name": tool_name, "table_name": table_name, "col_name": col_name}
                    }
                    yield _format_sse({
                        "step": f"Skipping tool for column: {col_name}", 
                        "details": skip_result, 
                        "tool_name": tool_name,
                        "type": "workaround"
                    }, "tool_result")
                    all_column_results.append(skip_result)
                    continue

            iter_args = base_args.copy()
            iter_args['col_name'] = col_name
            
            if db_name and table_name and '.' not in table_name:
                iter_args["table_name"] = f"{db_name}.{table_name}"
                if 'db_name' in iter_args: del iter_args["db_name"]

            iter_command = {"tool_name": tool_name, "arguments": iter_args}

            yield _format_sse({"step": "Tool Execution Intent", "details": iter_command}, "tool_result")
            col_result = await mcp_adapter.invoke_mcp_tool(self.dependencies['STATE'], iter_command)
            
            if 'notification' in iter_command:
                yield _format_sse({
                    "step": "System Notification", 
                    "details": iter_command['notification'],
                    "type": "workaround"
                })
                del iter_command['notification']

            if isinstance(col_result, dict) and col_result.get("error") == "parameter_mismatch":
                yield _format_sse({"details": col_result}, "request_user_input")
                self.state = AgentState.ERROR
                return

            yield _format_sse({"step": f"Tool Execution Result for column: {col_name}", "details": col_result, "tool_name": tool_name}, "tool_result")
            all_column_results.append(col_result)

        if self.iteration_context:
            ctx = self.iteration_context
            current_item_name = ctx["items"][ctx["item_index"]]
            ctx["results_per_item"][current_item_name].append(all_column_results)
            ctx["action_count_for_item"] += 1
        else:
            self.collected_data.append(all_column_results)
        
        tool_result_str = json.dumps({"tool_name": tool_name, "tool_output": all_column_results})

        yield _format_sse({"step": "Thinking about the next action...", "details": "Adaptive column iteration complete. Resuming main plan."})
        async for event in self._get_next_action_from_llm(tool_result_str=tool_result_str):
            yield event
    # --- END NEW ---

    async def _get_next_action_from_llm(self, tool_result_str: str | None = None):
        """
        Determines the appropriate prompt to send to the LLM to get the next
        action, based on the current state and the result of the last action.
        """
        prompt_for_next_step = "" 
        
        last_tool_failed = tool_result_str and '"error":' in tool_result_str.lower()
        object_does_not_exist = tool_result_str and "object does not exist" in tool_result_str.lower()

        if last_tool_failed and object_does_not_exist and not self.active_prompt_plan:
             prompt_for_next_step = (
                "The last tool call failed because the object (e.g., table) does not exist. This is a common error when the database name is missing.\n\n"
                "**CRITICAL RECOVERY INSTRUCTION:**\n"
                "1. **Review the conversation history.** Look for a recently mentioned database name (e.g., `DEMO_Customer360_db`).\n"
                "2. **If you find a database name,** your next action is to **retry the failed tool call**, but this time include the `db_name` argument with the value you found in the history.\n"
                "3. **If you cannot find a database name in the history,** then and only then should you ask the user for clarification by responding with `FINAL_ANSWER:`."
             )
        elif self.active_prompt_plan and not tool_result_str:
            app_logger.info("Applying forceful, plan-initiating reasoning for next step.")
            prompt_for_next_step = (
                "You have just been given a detailed, multi-phase plan. Your current task is to begin execution.\n\n"
                f"--- FULL PLAN ---\n{self.active_prompt_plan}\n\n"
                "--- YOUR IMMEDIATE TASK ---\n"
                "**Execute Phase 1, Step 1 of the plan.** Read the plan carefully and identify the very first tool you need to call. "
                "Your response MUST be a single JSON block for that tool call. Do not summarize or ask questions."
            )
        elif self.active_prompt_plan:
            app_logger.info("Applying forceful, plan-aware reasoning for next step.")
            last_tool_name = self.current_command.get("tool_name") if self.current_command else "N/A"
            prompt_for_next_step = (
                "You are in the middle of executing a multi-step plan. Your primary goal is to follow this plan to completion.\n\n"
                f"--- ORIGINAL PLAN ---\n{self.active_prompt_plan}\n\n"
                "--- CURRENT STATE ---\n"
                f"- You have just completed a step by executing the tool `{last_tool_name}`.\n"
                "- The result of this tool call is now in the conversation history.\n\n"
                "--- YOUR TASK: EXECUTE THE NEXT STEP ---\n"
                "1. **Analyze the ORIGINAL PLAN.** Determine the *very next* instruction in the sequence.\n"
                "2. **Execute that instruction.**\n"
                "   - If the next step is to call another tool, your response **MUST** be a single JSON block for that tool call.\n"
                "   - If the next step is the final text-generation phase of the plan (e.g., writing the final description), your response **MUST** start with `FINAL_ANSWER:`.\n\n"
                "**CRITICAL RULE on `FINAL_ANSWER`:** The `FINAL_ANSWER` keyword is reserved **exclusively** for the final, complete, user-facing response at the very end of the entire plan. "
                "Do **NOT** use `FINAL_ANSWER` for intermediate thoughts, status updates, or summaries of completed phases. If you are not delivering the final product to the user, your response must be a tool call."
            )
        elif self.iteration_context:
            ctx = self.iteration_context
            current_item_name = ctx["items"][ctx["item_index"]]
            
            if last_tool_failed:
                prompt_for_next_step = (
                    f"You are in the middle of an iterative plan for the item: **`{current_item_name}`**. "
                    "The last tool call failed. This is an expected failure for tools that only work on specific data types (e.g., statistics on numeric columns).\n\n"
                    "**CRITICAL INSTRUCTION:** Your task is to continue the plan. Acknowledge the minor error and proceed.\n"
                    "1. Look at the **ORIGINAL PLAN**.\n"
                    "2. Execute the **very next step** in the sequence for the **current item** (`{current_item_name}`).\n"
                    "3. **DO NOT** restart the plan. **DO NOT** skip to the next item."
                    f"\n\n--- ORIGINAL PLAN ---\n{self.active_prompt_plan}\n\n"
                )
            else:
                last_tool_table = self.current_command.get("arguments", {}).get("table_name", "")
                
                if last_tool_table and last_tool_table != current_item_name:
                    try:
                        new_index = ctx["items"].index(last_tool_table)
                        ctx["item_index"] = new_index
                        ctx["action_count_for_item"] = 1
                        current_item_name = ctx["items"][ctx["item_index"]]
                    except ValueError:
                        pass 

                if ctx["action_count_for_item"] >= 4:
                    ctx["item_index"] += 1
                    ctx["action_count_for_item"] = 0
                    if ctx["item_index"] >= len(ctx["items"]):
                        self.iteration_context = None
                        prompt_for_next_step = (
                            "You have successfully completed all steps for all items in the iterative phase of the plan. "
                            "All results are now in the conversation history. Your next and final task is to proceed to the next major phase of the **ORIGINAL PLAN** (Phase 3). "
                            "This final phase requires you to synthesize all the information you have gathered into a comprehensive dashboard or report. "
                            "This is a text generation task. Do not call any more tools. "
                            "Your response **MUST** start with `FINAL_ANSWER:`."
                        )
                    else:
                         current_item_name = ctx["items"][ctx["item_index"]]
                         prompt_for_next_step = (
                            f"You have finished all steps for the previous item. Now, begin Phase 2 for the **next item**: `{current_item_name}`. "
                            "According to the original plan, what is the first step for this new item?"
                        )
                else:
                    prompt_for_next_step = (
                        "You are executing a multi-step, iterative plan.\n\n"
                        f"--- ORIGINAL PLAN ---\n{self.active_prompt_plan}\n\n"
                        f"--- CURRENT FOCUS ---\n"
                        f"- You are working on item: **`{current_item_name}`**.\n"
                        f"- You have taken {ctx['action_count_for_item']} action(s) for this item so far.\n"
                        "- The result of the last action is in the conversation history.\n\n"
                        "--- YOUR NEXT TASK ---\n"
                        "1. Look at the **ORIGINAL PLAN** to see what the next step in the sequence is for the current item (`{current_item_name}`).\n"
                        "2. Execute the correct next step. Provide a tool call in a `json` block or perform the required text generation."
                    )
        else:
            prompt_for_next_step = (
                "You have just received data from a tool call. Review the data and your instructions to decide the next step.\n\n"
                "1.  **Consider a Chart:** Review the `--- Charting Rules ---` in your system prompt. Based on the data you just received, would a chart be an appropriate and helpful way to visualize the information for the user?\n\n"
                "2.  **Choose Your Action:**\n"
                "    -   If a chart is appropriate, your next action is to call the correct chart-generation tool. Respond with only a `Thought:` and a ```json...``` block for that tool.\n"
                "    -   If you still need more information from other tools, call the next appropriate tool by responding with a `Thought:` and a ```json...``` block.\n"
                "    -   If a chart is **not** appropriate and you have all the information needed to answer the user's request, you **MUST** provide the final answer. Your response **MUST** be plain text that starts with `FINAL_ANSWER:`. **DO NOT** use a JSON block for the final answer."
            )
        
        if tool_result_str:
            final_prompt_to_llm = f"{prompt_for_next_step}\n\nThe last tool execution returned the following result. Use this to inform your next action:\n\n{tool_result_str}"
        else:
            final_prompt_to_llm = prompt_for_next_step

        self.next_action_str, input_tokens, output_tokens = await llm_handler.call_llm_api(self.dependencies['STATE']['llm'], final_prompt_to_llm, self.session_id)
        yield _format_sse({"input": input_tokens, "output": output_tokens}, "llm_call_metrics")
        
        updated_session = session_manager.get_session(self.session_id)
        if updated_session:
            token_data = {
                "input_tokens": updated_session.get("input_tokens", 0),
                "output_tokens": updated_session.get("output_tokens", 0)
            }
            yield _format_sse(token_data, "token_update")
        
        if not self.next_action_str:
            raise ValueError("LLM failed to provide a response.")
        self.state = AgentState.DECIDING

    async def _handle_summarizing(self):
        """
        Generates the final summary answer after the plan is complete.
        It synthesizes all collected data and the LLM's final thoughts into a
        formatted HTML response.
        """
        llm_response = self.next_action_str
        summary_text = ""

        final_answer_match = re.search(r'FINAL_ANSWER:(.*)', llm_response, re.DOTALL | re.IGNORECASE)

        if final_answer_match:
            summary_text = final_answer_match.group(1).strip()
        else:
            yield _format_sse({"step": "Plan finished, generating final summary...", "details": "The agent is synthesizing all collected data."})
            final_prompt = (
                "You have executed a multi-step plan. All results are in the history. "
                f"Your final task is to synthesize this information into a comprehensive, natural language answer for the user's original request: '{self.original_user_input}'. "
                "Your response MUST start with `FINAL_ANSWER:`.\n\n"
                "**CRITICAL INSTRUCTIONS:**\n"
                "1. Provide a concise, user-focused summary in plain text or simple markdown.\n"
                "2. **DO NOT** include raw data, SQL code, or complex tables in your summary. The system will format and append this data automatically.\n"
                "3. If you see results with a 'skipped' status, you MUST mention this in your summary and explain that the tool was not applicable to that specific column or data type.\n"
                "4. Do not describe your internal thought process."
            )
            final_llm_response, input_tokens, output_tokens = await llm_handler.call_llm_api(self.dependencies['STATE']['llm'], final_prompt, self.session_id)
            yield _format_sse({"input": input_tokens, "output": output_tokens}, "llm_call_metrics")
            
            updated_session = session_manager.get_session(self.session_id)
            if updated_session:
                token_data = {
                    "input_tokens": updated_session.get("input_tokens", 0),
                    "output_tokens": updated_session.get("output_tokens", 0)
                }
                yield _format_sse(token_data, "token_update")

            final_answer_match_inner = re.search(r'FINAL_ANSWER:(.*)', final_llm_response or "", re.DOTALL | re.IGNORECASE)
            if final_answer_match_inner:
                summary_text = final_answer_match_inner.group(1).strip()
            else:
                summary_text = final_llm_response or "The agent finished its plan but did not provide a final summary."

        formatter = OutputFormatter(llm_summary_text=summary_text, collected_data=self.collected_data)
        final_html = formatter.render()
        session_manager.add_to_history(self.session_id, 'assistant', final_html)
        yield _format_sse({"final_answer": final_html}, "final_answer")
        self.state = AgentState.DONE

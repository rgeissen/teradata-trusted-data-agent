# trusted_data_agent/agent/executor.py
import re
import json
import logging
from enum import Enum, auto

from trusted_data_agent.agent.formatter import OutputFormatter
from trusted_data_agent.core import session_manager
from trusted_data_agent.mcp import adapter as mcp_adapter
from trusted_data_agent.llm import handler as llm_handler

app_logger = logging.getLogger("quart.app")

class AgentState(Enum):
    """
    Represents the current state of the agent's execution flow.
    """
    DECIDING = auto()
    EXECUTING_TOOL = auto()
    SUMMARIZING = auto()
    DONE = auto()
    ERROR = auto()

def _format_sse(data: dict, event: str = None) -> str:
    """
    Formats a dictionary into a Server-Sent Event (SSE) string.

    Args:
        data: The dictionary payload.
        event: An optional event name.

    Returns:
        A string formatted for SSE.
    """
    msg = f"data: {json.dumps(data)}\n"
    if event is not None:
        msg += f"event: {event}\n"
    return f"{msg}\n"

def _evaluate_inline_math(json_str: str) -> str:
    """
    Finds and evaluates simple mathematical expressions within a JSON string.
    This is used to handle cases where the LLM might generate chart parameters
    with inline calculations (e.g., "width": "100 * 2").

    Args:
        json_str: The JSON string to process.

    Returns:
        The JSON string with evaluated math expressions.
    """
    math_expr_pattern = re.compile(r'\b(\d+\.?\d*)\s*([+\-*/])\s*(\d+\.?\d*)\b')
    while True:
        match = math_expr_pattern.search(json_str)
        if not match: break
        num1_str, op, num2_str = match.groups()
        try:
            num1, num2 = float(num1_str), float(num2_str)
            result = 0
            if op == '+': result = num1 + num2
            elif op == '-': result = num1 - num2
            elif op == '*': result = num1 * num2
            elif op == '/': result = num1 / num2
            json_str = json_str.replace(match.group(0), str(result), 1)
        except (ValueError, ZeroDivisionError):
            # Ignore expressions that fail to evaluate
            break
    return json_str

class PlanExecutor:
    """
    Manages the step-by-step execution of a plan generated by the LLM.
    It handles the agent's state, tool execution, data collection, and final summary generation.
    """
    def __init__(self, session_id: str, initial_instruction: str, original_user_input: str, dependencies: dict):
        self.session_id = session_id
        self.original_user_input = original_user_input
        self.state = AgentState.DECIDING
        self.next_action_str = initial_instruction
        self.collected_data = []
        self.max_steps = 40  # Safety break to prevent infinite loops
        self.active_prompt_plan = None
        self.active_prompt_name = None
        self.current_command = None
        self.iteration_context = None
        self.dependencies = dependencies
        self.tool_constraints_cache = {} # Cache for inferred tool constraints
        self.globally_failed_tools = set() # Cache for tools that are fundamentally broken

    async def run(self):
        """
        Runs the main execution loop of the agent. This generator function
        yields Server-Sent Events (SSE) for each step of the process.
        """
        for i in range(self.max_steps):
            if self.state in [AgentState.DONE, AgentState.ERROR]:
                break
            try:
                if self.state == AgentState.DECIDING:
                    yield _format_sse({"step": "Assistant has decided on an action", "details": self.next_action_str}, "llm_thought")
                    async for event in self._handle_deciding():
                        yield event
                elif self.state == AgentState.EXECUTING_TOOL:
                    async for event in self._intercept_and_correct_command():
                        yield event
                    tool_name = self.current_command.get("tool_name")
                    
                    if tool_name in self.globally_failed_tools:
                        yield _format_sse({
                            "step": f"Skipping Globally Failed Tool: {tool_name}",
                            "details": f"The tool '{tool_name}' has been identified as non-functional for this session and will be skipped.",
                            "type": "workaround"
                        })
                        tool_result_str = json.dumps({ "tool_name": tool_name, "tool_output": { "status": "error", "error_message": "Skipped because tool is globally non-functional." } })
                        async for event_in_skip in self._get_next_action_from_llm(tool_result_str=tool_result_str):
                            yield event_in_skip
                        continue

                    tool_scopes = self.dependencies['STATE'].get('tool_scopes', {})
                    if tool_scopes.get(tool_name) == 'column':
                        async for event in self._execute_column_iteration():
                            yield event
                    else:
                        async for event in self._execute_standard_tool():
                            yield event

                elif self.state == AgentState.SUMMARIZING:
                    async for event in self._handle_summarizing():
                        yield event
            except Exception as e:
                app_logger.error(f"Error in state {self.state.name}: {e}", exc_info=True)
                self.state = AgentState.ERROR
                yield _format_sse({"error": "An error occurred during execution.", "details": str(e)}, "error")
        
        if self.state not in [AgentState.DONE, AgentState.ERROR]:
            async for event in self._handle_summarizing():
                yield event

    async def _intercept_and_correct_command(self):
        """
        Intercepts specific tool calls before execution and replaces them with
        a corrected, reliable alternative. This acts as a client-side patch
        for known faulty tools on the MCP server.
        """
        if not self.current_command: return

        tool_name = self.current_command.get("tool_name")

        if tool_name == "base_tableList":
            app_logger.warning("INTERCEPTED: Faulty tool 'base_tableList'. Replacing with 'base_readQuery'.")
            
            db_name = self.current_command.get("arguments", {}).get("db_name")
            if not db_name:
                raise ValueError("Cannot execute 'base_tableList' replacement: 'db_name' parameter is missing.")

            corrected_sql = f"SELECT TableName FROM DBC.TablesV WHERE DatabaseName = '{db_name}'"
            
            self.current_command = {
                "tool_name": "base_readQuery",
                "arguments": {"sql": corrected_sql}
            }
            
            yield _format_sse({
                "step": "System Correction",
                "details": f"Intercepted faulty 'base_tableList' tool. Replacing with a direct SQL query to ensure correct table filtering for database '{db_name}'.",
                "type": "workaround"
            })

    def _enrich_arguments_from_history(self, prompt_text: str, arguments: dict) -> tuple[dict, list]:
        """
        Enriches a given set of arguments with context from the conversation history
        to fill in any missing placeholders required by the prompt text.
        Returns the enriched arguments and a list of any SSE events to be yielded.
        """
        events_to_yield = []
        required_placeholders = re.findall(r'{(\w+)}', prompt_text)
        enriched_args = arguments.copy()
        
        alias_map = {
            "database_name": ["db_name"],
            "table_name": ["tbl_name", "object_name", "obj_name"],
            "column_name": ["col_name"]
        }
        reverse_alias_map = {alias: canon for canon, aliases in alias_map.items() for alias in aliases}

        for placeholder in required_placeholders:
            if placeholder in enriched_args:
                continue

            found_alias = False
            for alias, canon in reverse_alias_map.items():
                if canon == placeholder and alias in enriched_args:
                    enriched_args[placeholder] = enriched_args[alias]
                    found_alias = True
                    break
            if found_alias:
                continue

            app_logger.info(f"Placeholder '{placeholder}' is missing. Searching conversation history for context.")
            session_data = session_manager.get_session(self.session_id)
            if not session_data: continue

            for entry in reversed(session_data.get("generic_history", [])):
                content = entry.get("content")
                if not isinstance(content, str): continue
                
                try:
                    if entry.get("role") == "assistant":
                        json_match = re.search(r"```json\s*\n(.*?)\n\s*```", content, re.DOTALL)
                        if json_match:
                            command = json.loads(json_match.group(1).strip())
                            args_to_check = command.get("arguments", {})
                            if placeholder in args_to_check:
                                enriched_args[placeholder] = args_to_check[placeholder]
                                break
                            for alias, canon in reverse_alias_map.items():
                                if canon == placeholder and alias in args_to_check:
                                    enriched_args[placeholder] = args_to_check[alias]
                                    break
                            if placeholder in enriched_args: break
                except (json.JSONDecodeError, TypeError):
                    continue
            
            if placeholder in enriched_args:
                app_logger.info(f"Found value for '{placeholder}' in history: '{enriched_args[placeholder]}'")
                event = _format_sse({
                    "step": "System Correction",
                    "details": f"LLM omitted the '{placeholder}' parameter. The system inferred it from the conversation history.",
                    "type": "workaround"
                })
                events_to_yield.append(event)

        return enriched_args, events_to_yield

    async def _handle_deciding(self):
        """
        Parses the LLM's next action string to determine whether to execute a tool,
        follow a prompt, or generate the final answer.
        """
        if re.search(r'FINAL_ANSWER:', self.next_action_str, re.IGNORECASE):
            self.state = AgentState.SUMMARIZING
            return

        json_match = re.search(r"```json\s*\n(.*?)\n\s*```", self.next_action_str, re.DOTALL)
        if not json_match:
            if self.iteration_context:
                ctx = self.iteration_context
                current_item_name = ctx["items"][ctx["item_index"]]
                ctx["results_per_item"][current_item_name].append(self.next_action_str)
                ctx["action_count_for_item"] += 1
                async for event in self._get_next_action_from_llm():
                    yield event
                return

            app_logger.warning(f"LLM response not a tool command or FINAL_ANSWER. Summarizing. Response: {self.next_action_str}")
            self.state = AgentState.SUMMARIZING
            return

        command_str = json_match.group(1).strip()
        
        try:
            command = json.loads(command_str)
        except (json.JSONDecodeError, KeyError) as e:
            app_logger.error(f"JSON parsing failed. Error: {e}. Original string was: {command_str}")
            raise e
            
        if "tool_name" in command:
            t_name = command["tool_name"]
            mcp_tools = self.dependencies['STATE'].get('mcp_tools', {})
            mcp_prompts = self.dependencies['STATE'].get('mcp_prompts', {})
            if t_name not in mcp_tools and t_name in mcp_prompts:
                app_logger.warning(f"LLM hallucinated tool '{t_name}'. Correcting to prompt.")
                yield _format_sse({
                    "step": "System Correction",
                    "details": f"LLM incorrectly used 'tool_name' for a prompt. Corrected '{t_name}' to be a prompt.",
                    "type": "workaround"
                })
                command["prompt_name"] = command.pop("tool_name")
            
        self.current_command = command
        
        if "prompt_name" in command:
            prompt_name = command.get("prompt_name")
            
            mcp_client = self.dependencies['STATE'].get('mcp_client')
            if not mcp_client: raise RuntimeError("MCP client is not connected.")
            
            get_prompt_result = None
            async with mcp_client.session("teradata_mcp_server") as temp_session:
                get_prompt_result = await temp_session.get_prompt(name=prompt_name)
            
            if get_prompt_result is None: raise ValueError("Prompt retrieval from MCP server returned None.")
            prompt_text = get_prompt_result.content.text if hasattr(get_prompt_result, 'content') else str(get_prompt_result)

            is_workflow = "phase 1" in prompt_text.lower() and "cycle through" in prompt_text.lower()
            if is_workflow:
                app_logger.info(f"Workflow prompt '{prompt_name}' detected. Switching to algorithmic execution.")
                yield _format_sse({
                    "step": f"Workflow Detected: {prompt_name}",
                    "details": prompt_text,
                    "prompt_name": prompt_name
                }, "prompt_selected")
                async for event in self._parse_and_execute_workflow(prompt_text, command.get("arguments", {})):
                    yield event
                self.state = AgentState.SUMMARIZING
                return

            if self.active_prompt_plan:
                app_logger.warning(f"LLM attempted a nested prompt call. Forcing summarization.")
                self.state = AgentState.SUMMARIZING
                return

            self.active_prompt_name = prompt_name
            
            arguments = command.get("arguments", command.get("parameters", {}))
            
            enriched_arguments, events_to_yield = self._enrich_arguments_from_history(prompt_text, arguments)
            for event in events_to_yield:
                yield event

            self.active_prompt_plan = prompt_text.format(**enriched_arguments)
            yield _format_sse({"step": f"Executing Prompt: {prompt_name}", "details": self.active_prompt_plan, "prompt_name": prompt_name}, "prompt_selected")
            async for event in self._get_next_action_from_llm():
                yield event

        elif "tool_name" in command:
            self.state = AgentState.EXECUTING_TOOL
        else:
            self.state = AgentState.SUMMARIZING

    def _update_workflow_context(self, context: dict, tool_result: dict):
        """
        Generically inspects a tool result and updates the workflow context with derived facts.
        """
        if not isinstance(tool_result, dict): return
        
        results = tool_result.get("results")
        if not isinstance(results, list) or not results: return

        # Heuristic: Does this look like column summary data?
        first_row = results[0]
        if 'ColumnName' in first_row and 'NullCount' in first_row:
            app_logger.info(f"Context update: Found column summary data for table {context['table_name']}.")
            try:
                total_nulls = sum(int(row.get('NullCount', 1)) for row in results)
                context['total_nulls'] = total_nulls
                app_logger.info(f"Context update: Total nulls for table is {total_nulls}.")
            except (ValueError, TypeError):
                app_logger.warning("Could not calculate total nulls from summary data.")

    async def _should_skip_in_workflow(self, tool_name: str, context: dict) -> tuple[bool, str]:
        """
        Generically checks if a tool call should be skipped based on the current workflow context.
        """
        mcp_tools = self.dependencies['STATE'].get('mcp_tools', {})
        tool_spec = mcp_tools.get(tool_name)
        if not tool_spec:
            return False, ""

        # Heuristic 1: Skip tools related to missing/null values if we know there are none.
        tool_purpose = tool_spec.description.lower()
        is_about_nulls = 'missing' in tool_purpose or 'null' in tool_purpose
        
        if is_about_nulls and context.get('total_nulls') == 0:
            reason = f"Skipping because a previous step established that table '{context['table_name']}' has no null values."
            return True, reason

        return False, ""

    async def _parse_and_execute_workflow(self, prompt_text: str, args: dict):
        """
        Parses a workflow prompt into a deterministic plan and executes it algorithmically.
        """
        db_name = args.get("db_name") or args.get("database_name")
        if not db_name:
            raise ValueError("Workflow requires a database name.")

        # Phase 1: Get tables
        yield _format_sse({"step": "Workflow - Phase 1: Get Database Tables"})
        self.current_command = {"tool_name": "base_tableList", "arguments": {"db_name": db_name}}
        async for event in self._intercept_and_correct_command(): yield event
        
        table_list_result = await mcp_adapter.invoke_mcp_tool(self.dependencies['STATE'], self.current_command)
        self.collected_data.append(table_list_result)
        yield _format_sse({"step": "Tool Execution Result", "details": table_list_result, "tool_name": self.current_command.get("tool_name")}, "tool_result")
        
        tables = [item['TableName'] for item in table_list_result.get('results', [])]
        if not tables:
            yield _format_sse({"step": "Workflow Halted", "details": "No tables found in the database."})
            return

        tool_sequence_pattern = re.compile(r"using the `?(\w+)`? tool", re.IGNORECASE)
        phase_2_match = re.search(r"## Phase 2(.*?)## Phase 3", prompt_text, re.DOTALL | re.IGNORECASE)
        
        parsed_tool_sequence = []
        if phase_2_match:
            phase_2_text = phase_2_match.group(1)
            parsed_tool_sequence = tool_sequence_pattern.findall(phase_2_text)

        if not parsed_tool_sequence:
            raise ValueError("Workflow parsing failed: Could not determine tool sequence from prompt text.")
        
        resolved_tool_sequence = []
        all_valid_tools = self.dependencies['STATE'].get('mcp_tools', {}).keys()
        for parsed_name in parsed_tool_sequence:
            if parsed_name in all_valid_tools:
                resolved_tool_sequence.append(parsed_name)
                continue
            
            possible_matches = [valid_name for valid_name in all_valid_tools if parsed_name.endswith(valid_name)]
            
            if len(possible_matches) == 1:
                resolved_name = possible_matches[0]
                resolved_tool_sequence.append(resolved_name)
                yield _format_sse({
                    "step": "System Correction",
                    "details": f"Resolved ambiguous tool name '{parsed_name}' to '{resolved_name}' based on server capabilities.",
                    "type": "workaround"
                })
            else:
                raise ValueError(f"Workflow parsing failed: Tool name '{parsed_name}' from prompt is ambiguous or unknown.")
        
        app_logger.info(f"Dynamically parsed and resolved workflow sequence: {resolved_tool_sequence}")

        # Phase 2: Iterate through tables
        yield _format_sse({"step": f"Workflow - Phase 2: Collect Table Information for {len(tables)} tables"})
        for table_name in tables:
            yield _format_sse({"step": f"Processing Table: {table_name}"})
            
            table_workflow_context = {"table_name": table_name}

            for tool_name in resolved_tool_sequence:
                
                should_skip, reason = await self._should_skip_in_workflow(tool_name, table_workflow_context)
                if should_skip:
                    yield _format_sse({
                        "step": f"Workflow Step Skipped: {tool_name}",
                        "details": reason,
                        "type": "workaround"
                    })
                    self.collected_data.append({
                        "status": "skipped",
                        "reason": reason,
                        "metadata": {"tool_name": tool_name, "table_name": table_name}
                    })
                    continue

                self.current_command = {
                    "tool_name": tool_name,
                    "arguments": {"database_name": db_name, "table_name": table_name, "db_name": db_name}
                }

                if tool_name == "base_tableDDL":
                    ddl_result = await mcp_adapter.invoke_mcp_tool(self.dependencies['STATE'], self.current_command)
                    self.collected_data.append(ddl_result)
                    yield _format_sse({"step": "Tool Execution Result", "details": ddl_result, "tool_name": tool_name}, "tool_result")
                    ddl_text = ddl_result.get("results", [{}])[0].get("Request Text", "")
                    if ddl_text:
                        desc_prompt = (
                            "You are a data analyst. Your task is to provide a natural language business description based on a table's DDL.\n\n"
                            "**CRITICAL INSTRUCTIONS:**\n1. Your entire response MUST be plain text.\n2. DO NOT generate JSON, tool calls, or any code.\n3. Describe the likely business purpose of the table and each column.\n\n"
                            f"--- DDL to Analyze ---\n```sql\n{ddl_text}\n```"
                        )
                        description, _, _ = await llm_handler.call_llm_api(self.dependencies['STATE']['llm'], desc_prompt, self.session_id)
                        desc_result = {"type": "business_description", "table_name": table_name, "description": description, "metadata": {"tool_name": "llm_description_generation"}}
                        self.collected_data.append(desc_result)
                        yield _format_sse({"step": "Generated Business Description", "details": desc_result}, "tool_result")
                    continue

                tool_scopes = self.dependencies['STATE'].get('tool_scopes', {})
                if tool_scopes.get(tool_name) == 'column':
                    async for event in self._execute_column_iteration(callback_llm=False):
                        yield event
                    tool_result = self.collected_data[-1] if self.collected_data else {}
                else: 
                    tool_result = await mcp_adapter.invoke_mcp_tool(self.dependencies['STATE'], self.current_command)
                    self.collected_data.append(tool_result)
                    yield _format_sse({"step": "Tool Execution Result", "details": tool_result, "tool_name": tool_name}, "tool_result")

                self._update_workflow_context(table_workflow_context, tool_result)
        
        self.next_action_str = "FINAL_ANSWER:"
    
    async def _execute_standard_tool(self):
        """
        Invokes a standard (non-iterative) tool via the MCP adapter and
        handles the result.
        """
        yield _format_sse({"step": "Tool Execution Intent", "details": self.current_command}, "tool_result")
        tool_result = await mcp_adapter.invoke_mcp_tool(self.dependencies['STATE'], self.current_command)
        
        if 'notification' in self.current_command:
            yield _format_sse({
                "step": "System Notification", 
                "details": self.current_command['notification'],
                "type": "workaround"
            })
            del self.current_command['notification']

        tool_result_str = ""
        if isinstance(tool_result, dict) and "error" in tool_result:
            error_details = tool_result.get("data", tool_result.get("error", ""))
            
            if "Function" in str(error_details) and "does not exist" in str(error_details):
                tool_name = self.current_command.get("tool_name")
                self.globally_failed_tools.add(tool_name)
                app_logger.warning(f"Tool '{tool_name}' marked as globally failed for this session.")
            
            tool_result_str = json.dumps({ "tool_name": self.current_command.get("tool_name"), "tool_output": { "status": "error", "error_message": error_details } })
        else:
            tool_result_str = json.dumps({"tool_name": self.current_command.get("tool_name"), "tool_output": tool_result})
            if isinstance(tool_result, dict) and tool_result.get("type") == "chart":
                if self.collected_data:
                    app_logger.info("Chart generated. Removing previous data source from collected data to avoid duplicate display.")
                    self.collected_data.pop()
            self.collected_data.append(tool_result)

        if isinstance(tool_result, dict) and tool_result.get("error") == "parameter_mismatch":
            yield _format_sse({"details": col_result}, "request_user_input")
            self.state = AgentState.ERROR
            return

        yield _format_sse({"step": "Tool Execution Result", "details": tool_result, "tool_name": self.current_command.get("tool_name")}, "tool_result")

        if self.active_prompt_plan and not self.iteration_context:
            plan_text = self.active_prompt_plan.lower()
            is_iterative_plan = any(keyword in plan_text for keyword in ["cycle through", "for each", "iterate"])
            
            if is_iterative_plan and self.current_command.get("tool_name") == "base_readQuery" and isinstance(tool_result, dict) and tool_result.get("status") == "success":
                items_to_iterate = [res.get("TableName") for res in tool_result.get("results", []) if res.get("TableName")]
                if items_to_iterate:
                    self.iteration_context = { "items": items_to_iterate, "item_index": 0, "action_count_for_item": 0, "results_per_item": {item: [] for item in items_to_iterate} }
                    yield _format_sse({"step": "Starting Multi-Step Iteration", "details": f"Plan requires processing {len(items_to_iterate)} items."})
        
        yield _format_sse({"step": "Thinking about the next action...", "details": "The agent is reasoning based on the current context."})
        async for event in self._get_next_action_from_llm(tool_result_str=tool_result_str):
            yield event

    async def _get_tool_constraints(self, tool_name: str):
        """
        Infers and caches the operational constraints of a tool (e.g., data type)
        by using the LLM. This is a generator function that yields status updates
        and finally yields the constraint dictionary.
        """
        if tool_name in self.tool_constraints_cache:
            yield self.tool_constraints_cache[tool_name]
            return

        mcp_tools = self.dependencies['STATE'].get('mcp_tools', {})
        tool_definition = mcp_tools.get(tool_name)
        
        constraints = {}
        
        if tool_definition:
            prompt = (
                "You are a tool analyzer. Based on the tool's name and description, determine if its `col_name` "
                "argument is intended for 'numeric' types, 'character' types, or 'any' type. "
                "Respond with a single, raw JSON object with one key, 'dataType', and the value 'numeric', 'character', or 'any'.\n\n"
                f"Tool Name: `{tool_definition.name}`\n"
                f"Tool Description: \"{tool_definition.description}\"\n\n"
                "Example response for a statistical tool: {\"dataType\": \"numeric\"}"
            )
            
            yield _format_sse({
                "step": f"Inferring constraints for tool: {tool_name}",
                "details": "Asking LLM to analyze tool requirements...",
                "type": "workaround"
            })

            response_text, _, _ = await llm_handler.call_llm_api(
                self.dependencies['STATE']['llm'], 
                prompt, 
                raise_on_error=True,
                system_prompt_override="You are a JSON-only responding assistant."
            )
            
            try:
                match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if not match:
                    raise ValueError("LLM did not return valid JSON for constraints.")
                
                constraints = json.loads(match.group(0))
                app_logger.info(f"Inferred constraints for tool '{tool_name}': {constraints}")
            except (json.JSONDecodeError, ValueError) as e:
                app_logger.error(f"Failed to infer constraints for {tool_name}: {e}. Assuming no constraints.")
                constraints = {}
        
        self.tool_constraints_cache[tool_name] = constraints
        yield constraints

    async def _execute_column_iteration(self, column_subset: list = None, callback_llm: bool = True):
        """
        Handles the execution of tools that operate on a per-column basis.
        This version is adaptive: it fetches column metadata first (or uses cached data)
        and only runs tools on columns with a compatible data type based on LLM-inferred constraints.
        If `column_subset` is provided, it will only iterate over those columns.
        """
        base_command = self.current_command
        tool_name = base_command.get("tool_name")
        base_args = base_command.get("arguments", base_command.get("parameters", {}))
        
        db_name = base_args.get("database_name") or base_args.get("db_name")
        table_name = base_args.get("table_name") or base_args.get("obj_name")

        if table_name and '.' in table_name and not db_name:
            db_name, table_name = table_name.split('.', 1)
            app_logger.info(f"Parsed db_name '{db_name}' from fully qualified table_name.")

        specific_column = base_args.get("col_name") or base_args.get("column_name")
        if specific_column:
            yield _format_sse({"step": "Tool Execution Intent", "details": base_command}, "tool_result")
            col_result = await mcp_adapter.invoke_mcp_tool(self.dependencies['STATE'], base_command)
            
            if 'notification' in self.current_command:
                yield _format_sse({
                    "step": "System Notification", 
                    "details": self.current_command['notification'],
                    "type": "workaround"
                })
                del self.current_command['notification']

            if isinstance(col_result, dict) and col_result.get("error") == "parameter_mismatch":
                yield _format_sse({"details": col_result}, "request_user_input")
                self.state = AgentState.ERROR
                return

            yield _format_sse({"step": f"Tool Execution Result for column: {specific_column}", "details": col_result, "tool_name": tool_name}, "tool_result")
            self.collected_data.append(col_result)
            tool_result_str = json.dumps({"tool_name": tool_name, "tool_output": col_result})
            yield _format_sse({"step": "Thinking about the next action...", "details": "Single column execution complete. Resuming main plan."})
            if callback_llm:
                async for event in self._get_next_action_from_llm(tool_result_str=tool_result_str):
                    yield event
            return

        all_columns_metadata = []
        if column_subset:
            all_columns_metadata = [{"ColumnName": col_name, "DataType": "UNKNOWN"} for col_name in column_subset]
            app_logger.info(f"Executing column iteration on a pre-defined subset of {len(column_subset)} columns.")
        else:
            if self.collected_data:
                last_result = self.collected_data[-1]
                if isinstance(last_result, dict):
                    lr_meta = last_result.get("metadata", {})
                    if lr_meta.get("tool_name") in ["qlty_columnSummary", "base_columnDescription"] and lr_meta.get("table_name", "").endswith(table_name):
                        app_logger.info(f"Reusing column metadata from previous '{lr_meta.get('tool_name')}' tool call.")
                        all_columns_metadata = last_result.get("results", [])
            
            if not all_columns_metadata:
                yield _format_sse({
                    "step": f"Adaptive column tool detected: {tool_name}", 
                    "details": "Fetching column metadata to determine compatibility.",
                    "type": "workaround"
                })
                cols_command = {"tool_name": "base_columnDescription", "arguments": {"db_name": db_name, "obj_name": table_name}}
                cols_result = await mcp_adapter.invoke_mcp_tool(self.dependencies['STATE'], cols_command)

                if not (cols_result and isinstance(cols_result, dict) and cols_result.get('status') == 'success' and cols_result.get('results')):
                    raise ValueError(f"Failed to retrieve column list for iteration. Response: {cols_result}")
                all_columns_metadata = cols_result.get('results', [])

        all_column_results = []
        
        first_column_to_try = next((col.get("ColumnName") for col in all_columns_metadata), None)
        if first_column_to_try:
            preflight_args = base_args.copy()
            preflight_args['col_name'] = first_column_to_try
            preflight_command = {"tool_name": tool_name, "arguments": preflight_args}
            preflight_result = await mcp_adapter.invoke_mcp_tool(self.dependencies['STATE'], preflight_command)
            
            if isinstance(preflight_result, dict) and "error" in preflight_result:
                error_details = preflight_result.get("data", preflight_result.get("error", ""))
                if "Function" in str(error_details) and "does not exist" in str(error_details):
                    self.globally_failed_tools.add(tool_name)
                    app_logger.warning(f"Tool '{tool_name}' marked as globally failed by pre-flight check.")
                    yield _format_sse({ "step": f"Halting iteration for globally failed tool: {tool_name}", "details": "Pre-flight check failed.", "type": "error" })
                    all_column_results.append({ "status": "error", "reason": f"Tool '{tool_name}' is non-functional.", "metadata": {"tool_name": tool_name}})
                    self.collected_data.append(all_column_results)
                    return 

        tool_constraints = None
        if not column_subset or any(col.get("DataType") != "UNKNOWN" for col in all_columns_metadata):
            async for event in self._get_tool_constraints(tool_name):
                if isinstance(event, dict):
                    tool_constraints = event
                else:
                    yield event
        
        required_type = tool_constraints.get("dataType") if tool_constraints else None

        for column_info in all_columns_metadata:
            col_name = column_info.get("ColumnName")
            
            col_type = ""
            for key, value in column_info.items():
                if key.lower() in ["datatype", "columntype", "type"]:
                    col_type = (value or "").upper()
                    break
            if not col_type:
                for key, value in column_info.items():
                    if "type" in key.lower():
                        col_type = (value or "").upper()
                        break

            if required_type and col_type != "UNKNOWN":
                is_numeric_column = any(t in col_type for t in ["INT", "NUM", "DEC", "FLOAT", "BYTEINT", "SMALLINT", "BIGINT"])
                is_char_column = any(t in col_type for t in ["CHAR", "VARCHAR"])
                
                should_skip = False
                if required_type == "numeric" and not is_numeric_column:
                    should_skip = True
                elif required_type == "character" and not is_char_column:
                    should_skip = True

                if should_skip:
                    skip_reason = f"Tool '{tool_name}' requires a {required_type} column, but '{col_name}' is of type {col_type}."
                    app_logger.info(f"SKIPPING: {skip_reason}")
                    skip_result = {
                        "status": "skipped",
                        "reason": skip_reason,
                        "metadata": {"tool_name": tool_name, "table_name": table_name, "col_name": col_name}
                    }
                    yield _format_sse({
                        "step": f"Skipping tool for column: {col_name}", 
                        "details": skip_result, 
                        "tool_name": tool_name,
                        "type": "workaround"
                    }, "tool_result")
                    all_column_results.append(skip_result)
                    continue

            iter_args = base_args.copy()
            iter_args['col_name'] = col_name
            
            iter_command = {"tool_name": tool_name, "arguments": iter_args}

            yield _format_sse({"step": "Tool Execution Intent", "details": iter_command}, "tool_result")
            col_result = await mcp_adapter.invoke_mcp_tool(self.dependencies['STATE'], iter_command)
            
            if isinstance(col_result, dict) and "error" in col_result:
                error_details = col_result.get("data", col_result.get("error", ""))
                if "Function" in str(error_details) and "does not exist" in str(error_details):
                    self.globally_failed_tools.add(tool_name)
                    app_logger.warning(f"Tool '{tool_name}' marked as globally failed for this session during iteration.")
                    all_column_results.append({ "status": "error", "reason": f"Tool '{tool_name}' is non-functional.", "metadata": {"tool_name": tool_name}})
                    yield _format_sse({ "step": f"Halting iteration for globally failed tool: {tool_name}", "details": error_details, "type": "error" })
                    break 

            if 'notification' in iter_command:
                yield _format_sse({
                    "step": "System Notification", 
                    "details": iter_command['notification'],
                    "type": "workaround"
                })
                del iter_command['notification']

            if isinstance(col_result, dict) and col_result.get("error") == "parameter_mismatch":
                yield _format_sse({"details": col_result}, "request_user_input")
                self.state = AgentState.ERROR
                return

            yield _format_sse({"step": f"Tool Execution Result for column: {col_name}", "details": col_result, "tool_name": tool_name}, "tool_result")
            all_column_results.append(col_result)

        if self.iteration_context:
            ctx = self.iteration_context
            current_item_name = ctx["items"][ctx["item_index"]]
            ctx["results_per_item"][current_item_name].append(all_column_results)
            ctx["action_count_for_item"] += 1
        else:
            self.collected_data.append(all_column_results)
        
        tool_result_str = json.dumps({"tool_name": tool_name, "tool_output": all_column_results})

        yield _format_sse({"step": "Thinking about the next action...", "details": "Adaptive column iteration complete. Resuming main plan."})
        if callback_llm:
            async for event in self._get_next_action_from_llm(tool_result_str=tool_result_str):
                yield event

    async def _get_next_action_from_llm(self, tool_result_str: str | None = None):
        """
        Determines the appropriate prompt to send to the LLM to get the next
        action, based on the current state and the result of the last action.
        """
        prompt_for_next_step = "" 
        
        if self.active_prompt_plan:
            app_logger.info("Applying forceful, plan-aware reasoning for next step.")
            last_tool_name = self.current_command.get("tool_name") if self.current_command else "N/A"
            prompt_for_next_step = (
                "You are in the middle of executing a multi-step plan. Your primary goal is to follow this plan to completion.\n\n"
                f"--- ORIGINAL PLAN ---\n{self.active_prompt_plan}\n\n"
                "--- CURRENT STATE ---\n"
                f"- You have just completed a step by executing the tool `{last_tool_name}`.\n"
                "- The result of this tool call is now in the conversation history.\n\n"
                "--- YOUR TASK: EXECUTE THE NEXT STEP ---\n"
                "1. **Analyze the ORIGINAL PLAN.** Determine the *very next* instruction in the sequence.\n"
                "2. **Execute that instruction.**\n"
                "   - If the next step is to call another tool, your response **MUST** be a single JSON block for that tool call.\n"
                "   - If the next step is the final text-generation phase of the plan (e.g., writing the final description), your response **MUST** start with `FINAL_ANSWER:`.\n\n"
                "**CRITICAL RULE on `FINAL_ANSWER`:** The `FINAL_ANSWER` keyword is reserved **exclusively** for the final, complete, user-facing response at the very end of the entire plan. "
                "Do **NOT** use `FINAL_ANSWER` for intermediate thoughts, status updates, or summaries of completed phases. If you are not delivering the final product to the user, your response must be a tool call."
            )
        else:
            prompt_for_next_step = (
                "You have just received data from a tool call. Review the data and your instructions to decide the next step.\n\n"
                "1.  **Consider a Chart:** Review the `--- Charting Rules ---` in your system prompt. Based on the data you just received, would a chart be an appropriate and helpful way to visualize the information for the user?\n\n"
                "2.  **Choose Your Action:**\n"
                "    -   If a chart is appropriate, your next action is to call the correct chart-generation tool. Respond with only a `Thought:` and a ```json...``` block for that tool.\n"
                "    -   If you still need more information from other tools, call the next appropriate tool by responding with a `Thought:` and a ```json...``` block.\n"
                "    -   If a chart is **not** appropriate and you have all the information needed to answer the user's request, you **MUST** provide the final answer. Your response **MUST** be plain text that starts with `FINAL_ANSWER:`. **DO NOT** use a JSON block for the final answer."
            )
        
        if tool_result_str:
            final_prompt_to_llm = f"{prompt_for_next_step}\n\nThe last tool execution returned the following result. Use this to inform your next action:\n\n{tool_result_str}"
        else:
            final_prompt_to_llm = prompt_for_next_step

        self.next_action_str, input_tokens, output_tokens = await llm_handler.call_llm_api(self.dependencies['STATE']['llm'], final_prompt_to_llm, self.session_id)
        yield _format_sse({"input": input_tokens, "output": output_tokens}, "llm_call_metrics")
        
        updated_session = session_manager.get_session(self.session_id)
        if updated_session:
            token_data = {
                "input_tokens": updated_session.get("input_tokens", 0),
                "output_tokens": updated_session.get("output_tokens", 0)
            }
            yield _format_sse(token_data, "token_update")
        
        if not self.next_action_str:
            raise ValueError("LLM failed to provide a response.")
        self.state = AgentState.DECIDING

    async def _handle_summarizing(self):
        """
        Generates the final summary answer after the plan is complete.
        It synthesizes all collected data and the LLM's final thoughts into a
        formatted HTML response.
        """
        llm_response = self.next_action_str
        summary_text = ""

        final_answer_match = re.search(r'FINAL_ANSWER:(.*)', llm_response, re.DOTALL | re.IGNORECASE)

        if final_answer_match:
            summary_text = final_answer_match.group(1).strip()

        # If there was a FINAL_ANSWER tag but no text, or if there was no tag at all, generate the summary.
        if not summary_text:
            yield _format_sse({"step": "Plan finished, generating final summary...", "details": "The agent is synthesizing all collected data."})
            final_prompt = (
                "You have executed a multi-step plan. All results are in the history. "
                f"Your final task is to synthesize this information into a comprehensive, natural language answer for the user's original request: '{self.original_user_input}'. "
                "Your response MUST start with `FINAL_ANSWER:`.\n\n"
                "**CRITICAL INSTRUCTIONS:**\n"
                "1. Provide a concise, user-focused summary in plain text or simple markdown.\n"
                "2. **DO NOT** include raw data, SQL code, or complex tables in your summary. The system will format and append this data automatically.\n"
                "3. If you see results with a 'skipped' status, you MUST mention this in your summary and explain that the tool was not applicable to that specific column or data type.\n"
                "4. Do not describe your internal thought process."
            )
            final_llm_response, input_tokens, output_tokens = await llm_handler.call_llm_api(self.dependencies['STATE']['llm'], final_prompt, self.session_id)
            yield _format_sse({"input": input_tokens, "output": output_tokens}, "llm_call_metrics")
            
            updated_session = session_manager.get_session(self.session_id)
            if updated_session:
                token_data = {
                    "input_tokens": updated_session.get("input_tokens", 0),
                    "output_tokens": updated_session.get("output_tokens", 0)
                }
                yield _format_sse(token_data, "token_update")

            final_answer_match_inner = re.search(r'FINAL_ANSWER:(.*)', final_llm_response or "", re.DOTALL | re.IGNORECASE)
            if final_answer_match_inner:
                summary_text = final_answer_match_inner.group(1).strip()
            else:
                summary_text = final_llm_response or "The agent finished its plan but did not provide a final summary."

        if self.active_prompt_name == "qlty_databaseQuality":
            final_html = self._format_quality_workflow_summary(summary_text)
        else:
            formatter = OutputFormatter(llm_summary_text=summary_text, collected_data=self.collected_data)
            final_html = formatter.render()
        
        session_manager.add_to_history(self.session_id, 'assistant', final_html)
        yield _format_sse({"final_answer": final_html}, "final_answer")
        self.state = AgentState.DONE

    def _format_quality_workflow_summary(self, llm_summary: str) -> str:
        """
        A specialized formatter to render the results of the qlty_databaseQuality workflow.
        """
        data_by_table = {}
        for item in self.collected_data:
            if not isinstance(item, dict): continue
            
            metadata = item.get("metadata", {})
            tool_name = metadata.get("tool_name")
            
            if item.get("status") == "skipped":
                table_name = metadata.get("table_name")
                if not table_name: continue
            else:
                table_name = metadata.get("table_name", metadata.get("table"))

            if table_name and '.' in table_name:
                table_name = table_name.split('.')[1]

            if not table_name and item.get("type") == "business_description":
                table_name = item.get("table_name")

            if not table_name: continue

            if table_name not in data_by_table:
                data_by_table[table_name] = []
            data_by_table[table_name].append(item)

        html = f"<div class='response-card summary-card'><p>{llm_summary}</p></div>"
        
        formatter = OutputFormatter("", [])
        
        for table_name, data in data_by_table.items():
            html += f"<details class='response-card bg-white/5 open:pb-4 mb-4 rounded-lg border border-white/10'><summary class='p-4 font-bold text-xl text-white cursor-pointer hover:bg-white/10 rounded-t-lg'>Report for: <code>{table_name}</code></summary><div class='px-4'>"
            
            for item in data:
                tool_name = item.get("metadata", {}).get("tool_name")
                if item.get("type") == "business_description":
                    html += f"<div class='response-card'><h4 class='text-lg font-semibold text-white mb-2'>Business Description</h4><p class='text-gray-300'>{item.get('description')}</p></div>"
                elif tool_name == 'base_tableDDL':
                    html += formatter._render_ddl(item, 0)
                elif tool_name == 'qlty_columnSummary':
                    html += formatter._render_table(item, 0, f"Column Summary for {table_name}")
                elif tool_name == 'qlty_univariateStatistics':
                    if isinstance(item, list): 
                        flat_results = [res for col_res in item if isinstance(col_res, dict) and col_res.get('status') == 'success']
                        if flat_results:
                            combined_results = []
                            for res in flat_results:
                                combined_results.extend(res.get('results', []))
                            
                            if combined_results:
                                first_meta = flat_results[0].get('metadata', {})
                                html += formatter._render_table({"results": combined_results, "metadata": first_meta}, 0, f"Univariate Statistics for {table_name}")
                elif item.get("status") == "skipped":
                     html += f"<div class='response-card'><p class='text-sm text-gray-400 italic'>Skipped Step: <strong>{tool_name}</strong>. Reason: {item.get('reason')}</p></div>"


            html += "</div></details>"

        return html
